{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "2782634b-3c33-4ccc-8172-5b290c28574c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "7233b0ca-9d1d-4133-8769-3d9cd9866fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Hyperparams \n",
    "batch_size = 64\n",
    "num_workers = 4\n",
    "validation_ratio = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "4c4b4104-6e6d-46a3-b1ba-0b6ee2b5eac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5)),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "256fe92f-ca80-4106-8676-b7de48b019dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "10000\n",
      "40000\n"
     ]
    }
   ],
   "source": [
    "path = r'C:\\python\\datasets_storage'\n",
    "test_data = torchvision.datasets.CIFAR100(root=path,train=False,download=True, transform=transform)\n",
    "dataset_train = torchvision.datasets.CIFAR100(root=path,train=True,download=True, transform=transform)\n",
    "\n",
    "train_data,validation_data=torch.utils.data.random_split(dataset_train,[int((1-validation_ratio)*len(dataset_train)), int((validation_ratio)*len(dataset_train))])\n",
    "print(len(validation_data))\n",
    "print(len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "6ac711f6-648c-4a85-9966-45f3ca80e478",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_data, batch_size=batch_size, pin_memory=True, shuffle=True)\n",
    "val_loader = DataLoader(validation_data, batch_size=batch_size, pin_memory=True)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size,shuffle=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "35607460-4344-4279-be1a-a9fd36ce2afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_device():\n",
    "    if torch.cuda.is_available:\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "def move_device(tensor,device):\n",
    "    if isinstance(tensor,(list,tuple)):    \n",
    "      return[move_device(element,device) for element in tensor]  \n",
    "    return tensor.to(device,non_blocking=True)\n",
    "class DeviceDataloader():\n",
    "    def __init__(self,dataloader,device):\n",
    "        self.dataloader = dataloader\n",
    "        self.device = device\n",
    "    def __iter__(self):\n",
    "        # transfer ecach batch and return\n",
    "        for i in self.dataloader:\n",
    "           yield move_device(i, self.device)\n",
    "    def __len__(self):\n",
    "\n",
    "        return len(self.dataloader)\n",
    "device = check_device()\n",
    "\n",
    "train_dl = DeviceDataloader(train_loader, device)\n",
    "test_dl = DeviceDataloader(test_loader, device)\n",
    "val_dl  = DeviceDataloader(val_loader, device)\n",
    "#len(train_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "e6ac72ca-658e-4268-9098-4081aaa39bc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN_model(\n",
      "  (conv1): Sequential(\n",
      "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Dropout(p=0.3, inplace=False)\n",
      "  )\n",
      "  (conv2): Sequential(\n",
      "    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Dropout(p=0.3, inplace=False)\n",
      "  )\n",
      "  (conv3): Sequential(\n",
      "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Dropout(p=0.3, inplace=False)\n",
      "  )\n",
      "  (conv4): Sequential(\n",
      "    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Dropout(p=0.3, inplace=False)\n",
      "  )\n",
      "  (hidden_layer): Linear(in_features=1024, out_features=512, bias=True)\n",
      "  (output_layer): Linear(in_features=512, out_features=100, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class CNN_model(nn.Module):\n",
    "    def __init__(self, dropout_rate=0.3):\n",
    "        super(CNN_model,self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels = 3, out_channels = 32, kernel_size = 3, stride = 1, padding = 1),\n",
    "            nn.BatchNorm2d(32), # out channels, helps normalize batches and imporves overall better performance\n",
    "            nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size = 2),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels = 32, out_channels = 64, kernel_size = 3, stride = 1, padding = 1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size = 2),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels = 64, out_channels = 128, kernel_size = 3, stride = 1, padding = 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 2),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels = 128, out_channels = 256, kernel_size = 3, stride = 1, padding = 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 2),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )\n",
    "        self.hidden_layer = nn.Linear(256 * 2 * 2, 512)\n",
    "        self.output_layer = nn.Linear(512, 100)\n",
    "    def forward(self, x):\n",
    "        output = self.conv1(x)\n",
    "        output = self.conv2(output)\n",
    "        output = self.conv3(output)\n",
    "        output = self.conv4(output)\n",
    "        #flattening \n",
    "        output = output.view(-1, 256 * 2 * 2)\n",
    "        # fully connected layers\n",
    "        output = self.hidden_layer(output)\n",
    "        output = self.output_layer(output)\n",
    "        return output\n",
    "model = CNN_model()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "934698a3-11c3-4727-b342-4af0034eb89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'img_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "9eeb8c85-228f-4930-9e05-e2e3595089a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "#optimizer = optim.SGD(model.parameters(), lr= 3e-3, momentum=0.9)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=3e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "a5d6c0ae-fb48-48a9-a793-3f5256ab9112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "12.1\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "print(torch.version.cuda if torch.cuda.is_available() else \"CUDA not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "96a12572-66f4-48e3-81a7-54fd13358283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting...\n"
     ]
    }
   ],
   "source": [
    "print('Starting...')\n",
    "epochs = 55\n",
    "model = model.to(device)\n",
    "def model_train(model, train_loader, criterion, optimizer):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for batch_idx,(images,targets) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            images,targets = images.to(device), targets.to(device)\n",
    "            outputs = model(images)\n",
    "            \n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item() ### compares predicted lables to true lables and returns True/False\n",
    "            batch_idx += 1\n",
    "        \n",
    "            if batch_idx % 100 == 0:\n",
    "                print(f'Epoch\" {epoch + 1}/{epochs} | Loss:{running_loss} | '\n",
    "                      f'Loss: {running_loss / (batch_idx+1):.4f} | '\n",
    "                      f'Acc_train: {100.*correct/total:.2f}% | '\n",
    "                      f'Batch: {batch_idx / len(train_loader)}')\n",
    "def model_val(model, val_loader):\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, targets in val_loader:\n",
    "            with torch.no_grad():\n",
    "                images,targets = images.to(device), targets.to(device)\n",
    "                outputs = model(images)\n",
    "    \n",
    "                loss = criterion(outputs, targets)\n",
    "    \n",
    "                val_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                total += targets.size(0)\n",
    "                correct += predicted.eq(targets).sum().item()\n",
    "                return val_loss/len(val_loader), 100.* correct/total\n",
    "        print('Finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "8065a7dd-4e89-47e9-b09c-e66801e1de31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch\" 1/55 | Loss:492.31095838546753 | Loss: 4.8744 | Acc_train: 2.27% | Batch: 0.16\n",
      "Epoch\" 1/55 | Loss:930.823037147522 | Loss: 4.6310 | Acc_train: 3.09% | Batch: 0.32\n",
      "Epoch\" 1/55 | Loss:1357.230051279068 | Loss: 4.5091 | Acc_train: 3.89% | Batch: 0.48\n",
      "Epoch\" 1/55 | Loss:1773.321139574051 | Loss: 4.4222 | Acc_train: 4.82% | Batch: 0.64\n",
      "Epoch\" 1/55 | Loss:2181.2392127513885 | Loss: 4.3538 | Acc_train: 5.63% | Batch: 0.8\n",
      "Epoch\" 1/55 | Loss:2581.3448598384857 | Loss: 4.2951 | Acc_train: 6.33% | Batch: 0.96\n",
      "Epoch\" 2/55 | Loss:392.73014521598816 | Loss: 3.8884 | Acc_train: 12.73% | Batch: 0.16\n",
      "Epoch\" 2/55 | Loss:776.4895262718201 | Loss: 3.8631 | Acc_train: 13.61% | Batch: 0.32\n",
      "Epoch\" 2/55 | Loss:1158.5371389389038 | Loss: 3.8490 | Acc_train: 13.89% | Batch: 0.48\n",
      "Epoch\" 2/55 | Loss:1537.400114774704 | Loss: 3.8339 | Acc_train: 14.14% | Batch: 0.64\n",
      "Epoch\" 2/55 | Loss:1913.0721576213837 | Loss: 3.8185 | Acc_train: 14.33% | Batch: 0.8\n",
      "Epoch\" 2/55 | Loss:2283.771164894104 | Loss: 3.8000 | Acc_train: 14.74% | Batch: 0.96\n",
      "Epoch\" 3/55 | Loss:363.69886207580566 | Loss: 3.6010 | Acc_train: 18.75% | Batch: 0.16\n",
      "Epoch\" 3/55 | Loss:726.9775965213776 | Loss: 3.6168 | Acc_train: 18.83% | Batch: 0.32\n",
      "Epoch\" 3/55 | Loss:1088.9137523174286 | Loss: 3.6177 | Acc_train: 18.92% | Batch: 0.48\n",
      "Epoch\" 3/55 | Loss:1449.6668391227722 | Loss: 3.6151 | Acc_train: 19.32% | Batch: 0.64\n",
      "Epoch\" 3/55 | Loss:1806.5561950206757 | Loss: 3.6059 | Acc_train: 19.49% | Batch: 0.8\n",
      "Epoch\" 3/55 | Loss:2160.6141991615295 | Loss: 3.5950 | Acc_train: 19.63% | Batch: 0.96\n",
      "Epoch\" 4/55 | Loss:348.0943171977997 | Loss: 3.4465 | Acc_train: 22.03% | Batch: 0.16\n",
      "Epoch\" 4/55 | Loss:694.8953120708466 | Loss: 3.4572 | Acc_train: 22.32% | Batch: 0.32\n",
      "Epoch\" 4/55 | Loss:1040.942134141922 | Loss: 3.4583 | Acc_train: 22.41% | Batch: 0.48\n",
      "Epoch\" 4/55 | Loss:1386.6032660007477 | Loss: 3.4579 | Acc_train: 22.61% | Batch: 0.64\n",
      "Epoch\" 4/55 | Loss:1735.6550705432892 | Loss: 3.4644 | Acc_train: 22.53% | Batch: 0.8\n",
      "Epoch\" 4/55 | Loss:2077.151434659958 | Loss: 3.4562 | Acc_train: 22.82% | Batch: 0.96\n",
      "Epoch\" 5/55 | Loss:338.6789116859436 | Loss: 3.3533 | Acc_train: 24.55% | Batch: 0.16\n",
      "Epoch\" 5/55 | Loss:679.6694614887238 | Loss: 3.3814 | Acc_train: 24.01% | Batch: 0.32\n",
      "Epoch\" 5/55 | Loss:1017.6565387248993 | Loss: 3.3809 | Acc_train: 24.08% | Batch: 0.48\n",
      "Epoch\" 5/55 | Loss:1355.9685060977936 | Loss: 3.3815 | Acc_train: 24.34% | Batch: 0.64\n",
      "Epoch\" 5/55 | Loss:1692.8100910186768 | Loss: 3.3789 | Acc_train: 24.50% | Batch: 0.8\n",
      "Epoch\" 5/55 | Loss:2030.6116211414337 | Loss: 3.3787 | Acc_train: 24.58% | Batch: 0.96\n",
      "Epoch\" 6/55 | Loss:331.35133481025696 | Loss: 3.2807 | Acc_train: 26.53% | Batch: 0.16\n",
      "Epoch\" 6/55 | Loss:663.3492813110352 | Loss: 3.3002 | Acc_train: 26.26% | Batch: 0.32\n",
      "Epoch\" 6/55 | Loss:993.7699747085571 | Loss: 3.3016 | Acc_train: 26.29% | Batch: 0.48\n",
      "Epoch\" 6/55 | Loss:1324.002477645874 | Loss: 3.3018 | Acc_train: 26.40% | Batch: 0.64\n",
      "Epoch\" 6/55 | Loss:1653.6591255664825 | Loss: 3.3007 | Acc_train: 26.48% | Batch: 0.8\n",
      "Epoch\" 6/55 | Loss:1985.4145448207855 | Loss: 3.3035 | Acc_train: 26.37% | Batch: 0.96\n",
      "Epoch\" 7/55 | Loss:324.6946041584015 | Loss: 3.2148 | Acc_train: 26.67% | Batch: 0.16\n",
      "Epoch\" 7/55 | Loss:649.4824020862579 | Loss: 3.2313 | Acc_train: 27.31% | Batch: 0.32\n",
      "Epoch\" 7/55 | Loss:976.7058956623077 | Loss: 3.2449 | Acc_train: 27.28% | Batch: 0.48\n",
      "Epoch\" 7/55 | Loss:1304.458024263382 | Loss: 3.2530 | Acc_train: 27.42% | Batch: 0.64\n",
      "Epoch\" 7/55 | Loss:1632.3290011882782 | Loss: 3.2581 | Acc_train: 27.45% | Batch: 0.8\n",
      "Epoch\" 7/55 | Loss:1958.9698104858398 | Loss: 3.2595 | Acc_train: 27.44% | Batch: 0.96\n",
      "Epoch\" 8/55 | Loss:321.3596556186676 | Loss: 3.1818 | Acc_train: 29.28% | Batch: 0.16\n",
      "Epoch\" 8/55 | Loss:645.7725925445557 | Loss: 3.2128 | Acc_train: 28.77% | Batch: 0.32\n",
      "Epoch\" 8/55 | Loss:963.090567111969 | Loss: 3.1996 | Acc_train: 29.16% | Batch: 0.48\n",
      "Epoch\" 8/55 | Loss:1283.5504922866821 | Loss: 3.2009 | Acc_train: 29.13% | Batch: 0.64\n",
      "Epoch\" 8/55 | Loss:1609.3712725639343 | Loss: 3.2123 | Acc_train: 28.86% | Batch: 0.8\n",
      "Epoch\" 8/55 | Loss:1932.6346435546875 | Loss: 3.2157 | Acc_train: 28.77% | Batch: 0.96\n",
      "Epoch\" 9/55 | Loss:317.47629737854004 | Loss: 3.1433 | Acc_train: 30.22% | Batch: 0.16\n",
      "Epoch\" 9/55 | Loss:637.7695593833923 | Loss: 3.1730 | Acc_train: 29.62% | Batch: 0.32\n",
      "Epoch\" 9/55 | Loss:955.3930859565735 | Loss: 3.1741 | Acc_train: 29.50% | Batch: 0.48\n",
      "Epoch\" 9/55 | Loss:1272.1488513946533 | Loss: 3.1724 | Acc_train: 29.50% | Batch: 0.64\n",
      "Epoch\" 9/55 | Loss:1592.7708723545074 | Loss: 3.1792 | Acc_train: 29.37% | Batch: 0.8\n",
      "Epoch\" 9/55 | Loss:1913.4865987300873 | Loss: 3.1838 | Acc_train: 29.43% | Batch: 0.96\n",
      "Epoch\" 10/55 | Loss:318.202419757843 | Loss: 3.1505 | Acc_train: 29.08% | Batch: 0.16\n",
      "Epoch\" 10/55 | Loss:630.6009838581085 | Loss: 3.1373 | Acc_train: 29.66% | Batch: 0.32\n",
      "Epoch\" 10/55 | Loss:947.4331314563751 | Loss: 3.1476 | Acc_train: 29.81% | Batch: 0.48\n",
      "Epoch\" 10/55 | Loss:1262.5934097766876 | Loss: 3.1486 | Acc_train: 29.73% | Batch: 0.64\n",
      "Epoch\" 10/55 | Loss:1578.7810974121094 | Loss: 3.1513 | Acc_train: 29.72% | Batch: 0.8\n",
      "Epoch\" 10/55 | Loss:1894.236145734787 | Loss: 3.1518 | Acc_train: 29.86% | Batch: 0.96\n",
      "Epoch\" 11/55 | Loss:311.96863055229187 | Loss: 3.0888 | Acc_train: 31.38% | Batch: 0.16\n",
      "Epoch\" 11/55 | Loss:623.174566745758 | Loss: 3.1004 | Acc_train: 31.48% | Batch: 0.32\n",
      "Epoch\" 11/55 | Loss:934.6392788887024 | Loss: 3.1051 | Acc_train: 31.43% | Batch: 0.48\n",
      "Epoch\" 11/55 | Loss:1249.0806086063385 | Loss: 3.1149 | Acc_train: 31.23% | Batch: 0.64\n",
      "Epoch\" 11/55 | Loss:1562.315081357956 | Loss: 3.1184 | Acc_train: 31.16% | Batch: 0.8\n",
      "Epoch\" 11/55 | Loss:1874.6097445487976 | Loss: 3.1192 | Acc_train: 31.23% | Batch: 0.96\n",
      "Epoch\" 12/55 | Loss:310.35078024864197 | Loss: 3.0728 | Acc_train: 32.20% | Batch: 0.16\n",
      "Epoch\" 12/55 | Loss:620.3647990226746 | Loss: 3.0864 | Acc_train: 32.05% | Batch: 0.32\n",
      "Epoch\" 12/55 | Loss:927.9288165569305 | Loss: 3.0828 | Acc_train: 32.03% | Batch: 0.48\n",
      "Epoch\" 12/55 | Loss:1239.1167912483215 | Loss: 3.0901 | Acc_train: 32.05% | Batch: 0.64\n",
      "Epoch\" 12/55 | Loss:1551.751323223114 | Loss: 3.0973 | Acc_train: 31.91% | Batch: 0.8\n",
      "Epoch\" 12/55 | Loss:1862.2871398925781 | Loss: 3.0986 | Acc_train: 31.75% | Batch: 0.96\n",
      "Epoch\" 13/55 | Loss:305.5011959075928 | Loss: 3.0248 | Acc_train: 32.59% | Batch: 0.16\n",
      "Epoch\" 13/55 | Loss:612.0411682128906 | Loss: 3.0450 | Acc_train: 32.83% | Batch: 0.32\n",
      "Epoch\" 13/55 | Loss:920.3260774612427 | Loss: 3.0576 | Acc_train: 32.64% | Batch: 0.48\n",
      "Epoch\" 13/55 | Loss:1228.7977013587952 | Loss: 3.0643 | Acc_train: 32.50% | Batch: 0.64\n",
      "Epoch\" 13/55 | Loss:1535.2979564666748 | Loss: 3.0645 | Acc_train: 32.53% | Batch: 0.8\n",
      "Epoch\" 13/55 | Loss:1845.3259739875793 | Loss: 3.0704 | Acc_train: 32.45% | Batch: 0.96\n",
      "Epoch\" 14/55 | Loss:303.7139046192169 | Loss: 3.0071 | Acc_train: 33.72% | Batch: 0.16\n",
      "Epoch\" 14/55 | Loss:608.3967111110687 | Loss: 3.0268 | Acc_train: 33.49% | Batch: 0.32\n",
      "Epoch\" 14/55 | Loss:914.8110027313232 | Loss: 3.0392 | Acc_train: 33.40% | Batch: 0.48\n",
      "Epoch\" 14/55 | Loss:1221.3677952289581 | Loss: 3.0458 | Acc_train: 33.16% | Batch: 0.64\n",
      "Epoch\" 14/55 | Loss:1526.4568531513214 | Loss: 3.0468 | Acc_train: 33.18% | Batch: 0.8\n",
      "Epoch\" 14/55 | Loss:1833.4709944725037 | Loss: 3.0507 | Acc_train: 33.07% | Batch: 0.96\n",
      "Epoch\" 15/55 | Loss:298.38983631134033 | Loss: 2.9544 | Acc_train: 34.72% | Batch: 0.16\n",
      "Epoch\" 15/55 | Loss:601.546059846878 | Loss: 2.9928 | Acc_train: 33.80% | Batch: 0.32\n",
      "Epoch\" 15/55 | Loss:905.178811788559 | Loss: 3.0072 | Acc_train: 33.47% | Batch: 0.48\n",
      "Epoch\" 15/55 | Loss:1206.1496818065643 | Loss: 3.0079 | Acc_train: 33.65% | Batch: 0.64\n",
      "Epoch\" 15/55 | Loss:1510.2938611507416 | Loss: 3.0146 | Acc_train: 33.50% | Batch: 0.8\n",
      "Epoch\" 15/55 | Loss:1816.3340229988098 | Loss: 3.0222 | Acc_train: 33.34% | Batch: 0.96\n",
      "Epoch\" 16/55 | Loss:298.6185541152954 | Loss: 2.9566 | Acc_train: 34.44% | Batch: 0.16\n",
      "Epoch\" 16/55 | Loss:597.6364543437958 | Loss: 2.9733 | Acc_train: 34.49% | Batch: 0.32\n",
      "Epoch\" 16/55 | Loss:898.8148546218872 | Loss: 2.9861 | Acc_train: 34.27% | Batch: 0.48\n",
      "Epoch\" 16/55 | Loss:1200.1752300262451 | Loss: 2.9930 | Acc_train: 34.10% | Batch: 0.64\n",
      "Epoch\" 16/55 | Loss:1503.2078247070312 | Loss: 3.0004 | Acc_train: 34.00% | Batch: 0.8\n",
      "Epoch\" 16/55 | Loss:1809.966967344284 | Loss: 3.0116 | Acc_train: 33.79% | Batch: 0.96\n",
      "Epoch\" 17/55 | Loss:296.8872961997986 | Loss: 2.9395 | Acc_train: 35.00% | Batch: 0.16\n",
      "Epoch\" 17/55 | Loss:595.3948812484741 | Loss: 2.9622 | Acc_train: 35.08% | Batch: 0.32\n",
      "Epoch\" 17/55 | Loss:895.2072296142578 | Loss: 2.9741 | Acc_train: 34.74% | Batch: 0.48\n",
      "Epoch\" 17/55 | Loss:1198.661464214325 | Loss: 2.9892 | Acc_train: 34.45% | Batch: 0.64\n",
      "Epoch\" 17/55 | Loss:1497.5600152015686 | Loss: 2.9891 | Acc_train: 34.71% | Batch: 0.8\n",
      "Epoch\" 17/55 | Loss:1799.066677570343 | Loss: 2.9935 | Acc_train: 34.71% | Batch: 0.96\n",
      "Epoch\" 18/55 | Loss:294.4608488082886 | Loss: 2.9155 | Acc_train: 35.50% | Batch: 0.16\n",
      "Epoch\" 18/55 | Loss:594.2930085659027 | Loss: 2.9567 | Acc_train: 34.71% | Batch: 0.32\n",
      "Epoch\" 18/55 | Loss:893.8039436340332 | Loss: 2.9694 | Acc_train: 34.57% | Batch: 0.48\n",
      "Epoch\" 18/55 | Loss:1193.2303292751312 | Loss: 2.9756 | Acc_train: 34.55% | Batch: 0.64\n",
      "Epoch\" 18/55 | Loss:1492.0815913677216 | Loss: 2.9782 | Acc_train: 34.50% | Batch: 0.8\n",
      "Epoch\" 18/55 | Loss:1790.3614695072174 | Loss: 2.9790 | Acc_train: 34.53% | Batch: 0.96\n",
      "Epoch\" 19/55 | Loss:297.1734046936035 | Loss: 2.9423 | Acc_train: 34.89% | Batch: 0.16\n",
      "Epoch\" 19/55 | Loss:590.7505671977997 | Loss: 2.9391 | Acc_train: 35.36% | Batch: 0.32\n",
      "Epoch\" 19/55 | Loss:888.5041363239288 | Loss: 2.9518 | Acc_train: 35.06% | Batch: 0.48\n",
      "Epoch\" 19/55 | Loss:1185.327701330185 | Loss: 2.9559 | Acc_train: 35.04% | Batch: 0.64\n",
      "Epoch\" 19/55 | Loss:1487.4019088745117 | Loss: 2.9689 | Acc_train: 34.71% | Batch: 0.8\n",
      "Epoch\" 19/55 | Loss:1783.7764749526978 | Loss: 2.9680 | Acc_train: 34.66% | Batch: 0.96\n",
      "Epoch\" 20/55 | Loss:291.144935131073 | Loss: 2.8826 | Acc_train: 36.20% | Batch: 0.16\n",
      "Epoch\" 20/55 | Loss:586.7074778079987 | Loss: 2.9189 | Acc_train: 35.78% | Batch: 0.32\n",
      "Epoch\" 20/55 | Loss:882.9537742137909 | Loss: 2.9334 | Acc_train: 35.57% | Batch: 0.48\n",
      "Epoch\" 20/55 | Loss:1179.0137960910797 | Loss: 2.9402 | Acc_train: 35.67% | Batch: 0.64\n",
      "Epoch\" 20/55 | Loss:1475.5085661411285 | Loss: 2.9451 | Acc_train: 35.52% | Batch: 0.8\n",
      "Epoch\" 20/55 | Loss:1774.2068593502045 | Loss: 2.9521 | Acc_train: 35.29% | Batch: 0.96\n",
      "Epoch\" 21/55 | Loss:291.13752031326294 | Loss: 2.8825 | Acc_train: 36.41% | Batch: 0.16\n",
      "Epoch\" 21/55 | Loss:580.2495589256287 | Loss: 2.8868 | Acc_train: 36.72% | Batch: 0.32\n",
      "Epoch\" 21/55 | Loss:878.0759127140045 | Loss: 2.9172 | Acc_train: 36.14% | Batch: 0.48\n",
      "Epoch\" 21/55 | Loss:1172.5353515148163 | Loss: 2.9240 | Acc_train: 36.02% | Batch: 0.64\n",
      "Epoch\" 21/55 | Loss:1466.7078919410706 | Loss: 2.9276 | Acc_train: 36.04% | Batch: 0.8\n",
      "Epoch\" 21/55 | Loss:1765.1436834335327 | Loss: 2.9370 | Acc_train: 35.66% | Batch: 0.96\n",
      "Epoch\" 22/55 | Loss:288.7743239402771 | Loss: 2.8592 | Acc_train: 37.31% | Batch: 0.16\n",
      "Epoch\" 22/55 | Loss:579.1467111110687 | Loss: 2.8813 | Acc_train: 37.12% | Batch: 0.32\n",
      "Epoch\" 22/55 | Loss:871.687858581543 | Loss: 2.8960 | Acc_train: 36.62% | Batch: 0.48\n",
      "Epoch\" 22/55 | Loss:1165.670866727829 | Loss: 2.9069 | Acc_train: 36.31% | Batch: 0.64\n",
      "Epoch\" 22/55 | Loss:1460.0129408836365 | Loss: 2.9142 | Acc_train: 36.17% | Batch: 0.8\n",
      "Epoch\" 22/55 | Loss:1753.630172252655 | Loss: 2.9179 | Acc_train: 36.00% | Batch: 0.96\n",
      "Epoch\" 23/55 | Loss:291.8872718811035 | Loss: 2.8900 | Acc_train: 36.19% | Batch: 0.16\n",
      "Epoch\" 23/55 | Loss:585.7504255771637 | Loss: 2.9142 | Acc_train: 35.62% | Batch: 0.32\n",
      "Epoch\" 23/55 | Loss:877.0061264038086 | Loss: 2.9136 | Acc_train: 36.10% | Batch: 0.48\n",
      "Epoch\" 23/55 | Loss:1167.8169915676117 | Loss: 2.9123 | Acc_train: 36.41% | Batch: 0.64\n",
      "Epoch\" 23/55 | Loss:1457.6012301445007 | Loss: 2.9094 | Acc_train: 36.45% | Batch: 0.8\n",
      "Epoch\" 23/55 | Loss:1749.7143020629883 | Loss: 2.9113 | Acc_train: 36.46% | Batch: 0.96\n",
      "Epoch\" 24/55 | Loss:288.0070114135742 | Loss: 2.8516 | Acc_train: 37.44% | Batch: 0.16\n",
      "Epoch\" 24/55 | Loss:578.0950448513031 | Loss: 2.8761 | Acc_train: 37.48% | Batch: 0.32\n",
      "Epoch\" 24/55 | Loss:868.6425895690918 | Loss: 2.8859 | Acc_train: 37.32% | Batch: 0.48\n",
      "Epoch\" 24/55 | Loss:1159.5424087047577 | Loss: 2.8916 | Acc_train: 37.11% | Batch: 0.64\n",
      "Epoch\" 24/55 | Loss:1448.54798579216 | Loss: 2.8913 | Acc_train: 36.99% | Batch: 0.8\n",
      "Epoch\" 24/55 | Loss:1739.583081960678 | Loss: 2.8945 | Acc_train: 36.99% | Batch: 0.96\n",
      "Epoch\" 25/55 | Loss:285.17258739471436 | Loss: 2.8235 | Acc_train: 37.73% | Batch: 0.16\n",
      "Epoch\" 25/55 | Loss:572.793016910553 | Loss: 2.8497 | Acc_train: 37.48% | Batch: 0.32\n",
      "Epoch\" 25/55 | Loss:861.7096719741821 | Loss: 2.8628 | Acc_train: 37.18% | Batch: 0.48\n",
      "Epoch\" 25/55 | Loss:1152.9354853630066 | Loss: 2.8752 | Acc_train: 37.09% | Batch: 0.64\n",
      "Epoch\" 25/55 | Loss:1444.3403916358948 | Loss: 2.8829 | Acc_train: 36.85% | Batch: 0.8\n",
      "Epoch\" 25/55 | Loss:1736.030915260315 | Loss: 2.8886 | Acc_train: 36.84% | Batch: 0.96\n",
      "Epoch\" 26/55 | Loss:286.0692377090454 | Loss: 2.8324 | Acc_train: 37.34% | Batch: 0.16\n",
      "Epoch\" 26/55 | Loss:575.13019323349 | Loss: 2.8613 | Acc_train: 37.48% | Batch: 0.32\n",
      "Epoch\" 26/55 | Loss:862.4409363269806 | Loss: 2.8653 | Acc_train: 37.92% | Batch: 0.48\n",
      "Epoch\" 26/55 | Loss:1149.7946045398712 | Loss: 2.8673 | Acc_train: 37.75% | Batch: 0.64\n",
      "Epoch\" 26/55 | Loss:1440.6519689559937 | Loss: 2.8756 | Acc_train: 37.42% | Batch: 0.8\n",
      "Epoch\" 26/55 | Loss:1730.952882528305 | Loss: 2.8801 | Acc_train: 37.43% | Batch: 0.96\n",
      "Epoch\" 27/55 | Loss:285.71829867362976 | Loss: 2.8289 | Acc_train: 37.52% | Batch: 0.16\n",
      "Epoch\" 27/55 | Loss:571.9253036975861 | Loss: 2.8454 | Acc_train: 37.79% | Batch: 0.32\n",
      "Epoch\" 27/55 | Loss:856.2475140094757 | Loss: 2.8447 | Acc_train: 37.94% | Batch: 0.48\n",
      "Epoch\" 27/55 | Loss:1145.9375357627869 | Loss: 2.8577 | Acc_train: 37.96% | Batch: 0.64\n",
      "Epoch\" 27/55 | Loss:1432.6894161701202 | Loss: 2.8597 | Acc_train: 37.90% | Batch: 0.8\n",
      "Epoch\" 27/55 | Loss:1720.962265253067 | Loss: 2.8635 | Acc_train: 37.89% | Batch: 0.96\n",
      "Epoch\" 28/55 | Loss:286.80872440338135 | Loss: 2.8397 | Acc_train: 37.95% | Batch: 0.16\n",
      "Epoch\" 28/55 | Loss:571.8897414207458 | Loss: 2.8452 | Acc_train: 38.48% | Batch: 0.32\n",
      "Epoch\" 28/55 | Loss:859.4565532207489 | Loss: 2.8553 | Acc_train: 38.18% | Batch: 0.48\n",
      "Epoch\" 28/55 | Loss:1148.9818716049194 | Loss: 2.8653 | Acc_train: 37.76% | Batch: 0.64\n",
      "Epoch\" 28/55 | Loss:1434.875888824463 | Loss: 2.8640 | Acc_train: 37.75% | Batch: 0.8\n",
      "Epoch\" 28/55 | Loss:1723.3633661270142 | Loss: 2.8675 | Acc_train: 37.64% | Batch: 0.96\n",
      "Epoch\" 29/55 | Loss:284.05223751068115 | Loss: 2.8124 | Acc_train: 38.45% | Batch: 0.16\n",
      "Epoch\" 29/55 | Loss:565.9123365879059 | Loss: 2.8155 | Acc_train: 38.37% | Batch: 0.32\n",
      "Epoch\" 29/55 | Loss:855.0509965419769 | Loss: 2.8407 | Acc_train: 38.01% | Batch: 0.48\n",
      "Epoch\" 29/55 | Loss:1140.221489906311 | Loss: 2.8434 | Acc_train: 37.95% | Batch: 0.64\n",
      "Epoch\" 29/55 | Loss:1424.8632154464722 | Loss: 2.8440 | Acc_train: 37.97% | Batch: 0.8\n",
      "Epoch\" 29/55 | Loss:1712.8872323036194 | Loss: 2.8501 | Acc_train: 37.85% | Batch: 0.96\n",
      "Epoch\" 30/55 | Loss:282.7856922149658 | Loss: 2.7999 | Acc_train: 39.19% | Batch: 0.16\n",
      "Epoch\" 30/55 | Loss:567.6272492408752 | Loss: 2.8240 | Acc_train: 38.62% | Batch: 0.32\n",
      "Epoch\" 30/55 | Loss:848.8525056838989 | Loss: 2.8201 | Acc_train: 38.66% | Batch: 0.48\n",
      "Epoch\" 30/55 | Loss:1135.8862655162811 | Loss: 2.8326 | Acc_train: 38.30% | Batch: 0.64\n",
      "Epoch\" 30/55 | Loss:1419.8952505588531 | Loss: 2.8341 | Acc_train: 38.18% | Batch: 0.8\n",
      "Epoch\" 30/55 | Loss:1705.6446313858032 | Loss: 2.8380 | Acc_train: 38.12% | Batch: 0.96\n",
      "Epoch\" 31/55 | Loss:281.139803647995 | Loss: 2.7836 | Acc_train: 39.23% | Batch: 0.16\n",
      "Epoch\" 31/55 | Loss:562.749413728714 | Loss: 2.7997 | Acc_train: 39.62% | Batch: 0.32\n",
      "Epoch\" 31/55 | Loss:849.0322136878967 | Loss: 2.8207 | Acc_train: 38.91% | Batch: 0.48\n",
      "Epoch\" 31/55 | Loss:1136.4782931804657 | Loss: 2.8341 | Acc_train: 38.51% | Batch: 0.64\n",
      "Epoch\" 31/55 | Loss:1419.7119925022125 | Loss: 2.8338 | Acc_train: 38.71% | Batch: 0.8\n",
      "Epoch\" 31/55 | Loss:1706.6559038162231 | Loss: 2.8397 | Acc_train: 38.58% | Batch: 0.96\n",
      "Epoch\" 32/55 | Loss:281.5607359409332 | Loss: 2.7877 | Acc_train: 38.91% | Batch: 0.16\n",
      "Epoch\" 32/55 | Loss:564.7269339561462 | Loss: 2.8096 | Acc_train: 38.73% | Batch: 0.32\n",
      "Epoch\" 32/55 | Loss:849.2466428279877 | Loss: 2.8214 | Acc_train: 38.69% | Batch: 0.48\n",
      "Epoch\" 32/55 | Loss:1133.782681941986 | Loss: 2.8274 | Acc_train: 38.55% | Batch: 0.64\n",
      "Epoch\" 32/55 | Loss:1418.3888120651245 | Loss: 2.8311 | Acc_train: 38.53% | Batch: 0.8\n",
      "Epoch\" 32/55 | Loss:1701.6085698604584 | Loss: 2.8313 | Acc_train: 38.65% | Batch: 0.96\n",
      "Epoch\" 33/55 | Loss:278.79735112190247 | Loss: 2.7604 | Acc_train: 39.83% | Batch: 0.16\n",
      "Epoch\" 33/55 | Loss:560.2298953533173 | Loss: 2.7872 | Acc_train: 39.86% | Batch: 0.32\n",
      "Epoch\" 33/55 | Loss:842.0815243721008 | Loss: 2.7976 | Acc_train: 39.46% | Batch: 0.48\n",
      "Epoch\" 33/55 | Loss:1127.3898520469666 | Loss: 2.8114 | Acc_train: 39.23% | Batch: 0.64\n",
      "Epoch\" 33/55 | Loss:1412.8436782360077 | Loss: 2.8200 | Acc_train: 39.09% | Batch: 0.8\n",
      "Epoch\" 33/55 | Loss:1697.524296283722 | Loss: 2.8245 | Acc_train: 38.93% | Batch: 0.96\n",
      "Epoch\" 34/55 | Loss:279.88482785224915 | Loss: 2.7711 | Acc_train: 39.98% | Batch: 0.16\n",
      "Epoch\" 34/55 | Loss:560.5389955043793 | Loss: 2.7888 | Acc_train: 39.62% | Batch: 0.32\n",
      "Epoch\" 34/55 | Loss:845.0987672805786 | Loss: 2.8076 | Acc_train: 39.15% | Batch: 0.48\n",
      "Epoch\" 34/55 | Loss:1124.9858558177948 | Loss: 2.8055 | Acc_train: 39.19% | Batch: 0.64\n",
      "Epoch\" 34/55 | Loss:1410.6746168136597 | Loss: 2.8157 | Acc_train: 38.99% | Batch: 0.8\n",
      "Epoch\" 34/55 | Loss:1691.140109539032 | Loss: 2.8139 | Acc_train: 39.03% | Batch: 0.96\n",
      "Epoch\" 35/55 | Loss:280.87965750694275 | Loss: 2.7810 | Acc_train: 39.77% | Batch: 0.16\n",
      "Epoch\" 35/55 | Loss:560.828676700592 | Loss: 2.7902 | Acc_train: 39.55% | Batch: 0.32\n",
      "Epoch\" 35/55 | Loss:839.1284673213959 | Loss: 2.7878 | Acc_train: 39.85% | Batch: 0.48\n",
      "Epoch\" 35/55 | Loss:1119.8454520702362 | Loss: 2.7926 | Acc_train: 39.82% | Batch: 0.64\n",
      "Epoch\" 35/55 | Loss:1402.3165316581726 | Loss: 2.7990 | Acc_train: 39.77% | Batch: 0.8\n",
      "Epoch\" 35/55 | Loss:1683.0653846263885 | Loss: 2.8004 | Acc_train: 39.70% | Batch: 0.96\n",
      "Epoch\" 36/55 | Loss:278.1509099006653 | Loss: 2.7540 | Acc_train: 40.23% | Batch: 0.16\n",
      "Epoch\" 36/55 | Loss:556.6000919342041 | Loss: 2.7692 | Acc_train: 40.16% | Batch: 0.32\n",
      "Epoch\" 36/55 | Loss:838.1539077758789 | Loss: 2.7846 | Acc_train: 39.84% | Batch: 0.48\n",
      "Epoch\" 36/55 | Loss:1118.9695751667023 | Loss: 2.7904 | Acc_train: 39.72% | Batch: 0.64\n",
      "Epoch\" 36/55 | Loss:1402.8197991847992 | Loss: 2.8000 | Acc_train: 39.46% | Batch: 0.8\n",
      "Epoch\" 36/55 | Loss:1684.6950843334198 | Loss: 2.8032 | Acc_train: 39.38% | Batch: 0.96\n",
      "Epoch\" 37/55 | Loss:276.3546817302704 | Loss: 2.7362 | Acc_train: 40.62% | Batch: 0.16\n",
      "Epoch\" 37/55 | Loss:555.5773198604584 | Loss: 2.7641 | Acc_train: 40.65% | Batch: 0.32\n",
      "Epoch\" 37/55 | Loss:836.819415807724 | Loss: 2.7801 | Acc_train: 40.26% | Batch: 0.48\n",
      "Epoch\" 37/55 | Loss:1119.5739169120789 | Loss: 2.7920 | Acc_train: 39.93% | Batch: 0.64\n",
      "Epoch\" 37/55 | Loss:1400.5036034584045 | Loss: 2.7954 | Acc_train: 39.85% | Batch: 0.8\n",
      "Epoch\" 37/55 | Loss:1683.4632835388184 | Loss: 2.8011 | Acc_train: 39.68% | Batch: 0.96\n",
      "Epoch\" 38/55 | Loss:272.5262522697449 | Loss: 2.6983 | Acc_train: 40.45% | Batch: 0.16\n",
      "Epoch\" 38/55 | Loss:552.5767977237701 | Loss: 2.7491 | Acc_train: 39.80% | Batch: 0.32\n",
      "Epoch\" 38/55 | Loss:833.1960124969482 | Loss: 2.7681 | Acc_train: 39.55% | Batch: 0.48\n",
      "Epoch\" 38/55 | Loss:1109.8297443389893 | Loss: 2.7677 | Acc_train: 39.77% | Batch: 0.64\n",
      "Epoch\" 38/55 | Loss:1391.0979688167572 | Loss: 2.7766 | Acc_train: 39.60% | Batch: 0.8\n",
      "Epoch\" 38/55 | Loss:1672.0940635204315 | Loss: 2.7822 | Acc_train: 39.60% | Batch: 0.96\n",
      "Epoch\" 39/55 | Loss:275.0312874317169 | Loss: 2.7231 | Acc_train: 41.09% | Batch: 0.16\n",
      "Epoch\" 39/55 | Loss:551.867527961731 | Loss: 2.7456 | Acc_train: 40.95% | Batch: 0.32\n",
      "Epoch\" 39/55 | Loss:832.8960185050964 | Loss: 2.7671 | Acc_train: 40.41% | Batch: 0.48\n",
      "Epoch\" 39/55 | Loss:1112.5616040229797 | Loss: 2.7745 | Acc_train: 40.23% | Batch: 0.64\n",
      "Epoch\" 39/55 | Loss:1389.8198273181915 | Loss: 2.7741 | Acc_train: 40.20% | Batch: 0.8\n",
      "Epoch\" 39/55 | Loss:1669.0309150218964 | Loss: 2.7771 | Acc_train: 40.11% | Batch: 0.96\n",
      "Epoch\" 40/55 | Loss:274.0646448135376 | Loss: 2.7135 | Acc_train: 41.14% | Batch: 0.16\n",
      "Epoch\" 40/55 | Loss:551.0880908966064 | Loss: 2.7417 | Acc_train: 40.40% | Batch: 0.32\n",
      "Epoch\" 40/55 | Loss:829.867951631546 | Loss: 2.7570 | Acc_train: 40.23% | Batch: 0.48\n",
      "Epoch\" 40/55 | Loss:1106.6444067955017 | Loss: 2.7597 | Acc_train: 40.33% | Batch: 0.64\n",
      "Epoch\" 40/55 | Loss:1386.4871158599854 | Loss: 2.7674 | Acc_train: 40.04% | Batch: 0.8\n",
      "Epoch\" 40/55 | Loss:1667.009400844574 | Loss: 2.7737 | Acc_train: 39.96% | Batch: 0.96\n",
      "Epoch\" 41/55 | Loss:274.230437040329 | Loss: 2.7152 | Acc_train: 41.16% | Batch: 0.16\n",
      "Epoch\" 41/55 | Loss:553.5451731681824 | Loss: 2.7540 | Acc_train: 40.25% | Batch: 0.32\n",
      "Epoch\" 41/55 | Loss:833.8173060417175 | Loss: 2.7702 | Acc_train: 40.15% | Batch: 0.48\n",
      "Epoch\" 41/55 | Loss:1111.8322060108185 | Loss: 2.7726 | Acc_train: 40.23% | Batch: 0.64\n",
      "Epoch\" 41/55 | Loss:1386.3586521148682 | Loss: 2.7672 | Acc_train: 40.35% | Batch: 0.8\n",
      "Epoch\" 41/55 | Loss:1667.378930568695 | Loss: 2.7743 | Acc_train: 40.15% | Batch: 0.96\n",
      "Epoch\" 42/55 | Loss:275.1132130622864 | Loss: 2.7239 | Acc_train: 41.00% | Batch: 0.16\n",
      "Epoch\" 42/55 | Loss:549.8398592472076 | Loss: 2.7355 | Acc_train: 40.95% | Batch: 0.32\n",
      "Epoch\" 42/55 | Loss:825.1411604881287 | Loss: 2.7413 | Acc_train: 41.09% | Batch: 0.48\n",
      "Epoch\" 42/55 | Loss:1103.5235526561737 | Loss: 2.7519 | Acc_train: 40.79% | Batch: 0.64\n",
      "Epoch\" 42/55 | Loss:1383.3811457157135 | Loss: 2.7612 | Acc_train: 40.54% | Batch: 0.8\n",
      "Epoch\" 42/55 | Loss:1660.974561214447 | Loss: 2.7637 | Acc_train: 40.44% | Batch: 0.96\n",
      "Epoch\" 43/55 | Loss:272.28101563453674 | Loss: 2.6959 | Acc_train: 41.64% | Batch: 0.16\n",
      "Epoch\" 43/55 | Loss:552.6513373851776 | Loss: 2.7495 | Acc_train: 40.75% | Batch: 0.32\n",
      "Epoch\" 43/55 | Loss:828.4706094264984 | Loss: 2.7524 | Acc_train: 40.79% | Batch: 0.48\n",
      "Epoch\" 43/55 | Loss:1105.0125472545624 | Loss: 2.7556 | Acc_train: 40.83% | Batch: 0.64\n",
      "Epoch\" 43/55 | Loss:1382.261031627655 | Loss: 2.7590 | Acc_train: 40.68% | Batch: 0.8\n",
      "Epoch\" 43/55 | Loss:1661.8993117809296 | Loss: 2.7652 | Acc_train: 40.53% | Batch: 0.96\n",
      "Epoch\" 44/55 | Loss:274.5119721889496 | Loss: 2.7179 | Acc_train: 41.20% | Batch: 0.16\n",
      "Epoch\" 44/55 | Loss:549.8443605899811 | Loss: 2.7355 | Acc_train: 41.05% | Batch: 0.32\n",
      "Epoch\" 44/55 | Loss:828.8321824073792 | Loss: 2.7536 | Acc_train: 40.78% | Batch: 0.48\n",
      "Epoch\" 44/55 | Loss:1107.1584556102753 | Loss: 2.7610 | Acc_train: 40.61% | Batch: 0.64\n",
      "Epoch\" 44/55 | Loss:1382.2292490005493 | Loss: 2.7589 | Acc_train: 40.74% | Batch: 0.8\n",
      "Epoch\" 44/55 | Loss:1656.914920091629 | Loss: 2.7569 | Acc_train: 40.80% | Batch: 0.96\n",
      "Epoch\" 45/55 | Loss:271.65177726745605 | Loss: 2.6896 | Acc_train: 42.03% | Batch: 0.16\n",
      "Epoch\" 45/55 | Loss:545.4114882946014 | Loss: 2.7135 | Acc_train: 42.12% | Batch: 0.32\n",
      "Epoch\" 45/55 | Loss:823.6004576683044 | Loss: 2.7362 | Acc_train: 41.62% | Batch: 0.48\n",
      "Epoch\" 45/55 | Loss:1101.716152191162 | Loss: 2.7474 | Acc_train: 41.41% | Batch: 0.64\n",
      "Epoch\" 45/55 | Loss:1376.848133802414 | Loss: 2.7482 | Acc_train: 41.33% | Batch: 0.8\n",
      "Epoch\" 45/55 | Loss:1656.1662046909332 | Loss: 2.7557 | Acc_train: 40.96% | Batch: 0.96\n",
      "Epoch\" 46/55 | Loss:272.649352312088 | Loss: 2.6995 | Acc_train: 41.97% | Batch: 0.16\n",
      "Epoch\" 46/55 | Loss:545.9130389690399 | Loss: 2.7160 | Acc_train: 42.08% | Batch: 0.32\n",
      "Epoch\" 46/55 | Loss:819.66863489151 | Loss: 2.7232 | Acc_train: 41.84% | Batch: 0.48\n",
      "Epoch\" 46/55 | Loss:1092.812902212143 | Loss: 2.7252 | Acc_train: 41.82% | Batch: 0.64\n",
      "Epoch\" 46/55 | Loss:1370.1438653469086 | Loss: 2.7348 | Acc_train: 41.64% | Batch: 0.8\n",
      "Epoch\" 46/55 | Loss:1648.8753480911255 | Loss: 2.7436 | Acc_train: 41.37% | Batch: 0.96\n",
      "Epoch\" 47/55 | Loss:271.0889046192169 | Loss: 2.6840 | Acc_train: 41.62% | Batch: 0.16\n",
      "Epoch\" 47/55 | Loss:545.821799993515 | Loss: 2.7155 | Acc_train: 41.45% | Batch: 0.32\n",
      "Epoch\" 47/55 | Loss:820.2940049171448 | Loss: 2.7252 | Acc_train: 41.51% | Batch: 0.48\n",
      "Epoch\" 47/55 | Loss:1095.0136868953705 | Loss: 2.7307 | Acc_train: 41.72% | Batch: 0.64\n",
      "Epoch\" 47/55 | Loss:1370.2998669147491 | Loss: 2.7351 | Acc_train: 41.44% | Batch: 0.8\n",
      "Epoch\" 47/55 | Loss:1647.511299610138 | Loss: 2.7413 | Acc_train: 41.18% | Batch: 0.96\n",
      "Epoch\" 48/55 | Loss:270.9714767932892 | Loss: 2.6829 | Acc_train: 41.80% | Batch: 0.16\n",
      "Epoch\" 48/55 | Loss:545.9170689582825 | Loss: 2.7160 | Acc_train: 41.52% | Batch: 0.32\n",
      "Epoch\" 48/55 | Loss:822.2366993427277 | Loss: 2.7317 | Acc_train: 41.13% | Batch: 0.48\n",
      "Epoch\" 48/55 | Loss:1096.747507095337 | Loss: 2.7350 | Acc_train: 41.15% | Batch: 0.64\n",
      "Epoch\" 48/55 | Loss:1370.7038595676422 | Loss: 2.7359 | Acc_train: 40.98% | Batch: 0.8\n",
      "Epoch\" 48/55 | Loss:1648.3810255527496 | Loss: 2.7427 | Acc_train: 40.83% | Batch: 0.96\n",
      "Epoch\" 49/55 | Loss:270.1796541213989 | Loss: 2.6750 | Acc_train: 42.42% | Batch: 0.16\n",
      "Epoch\" 49/55 | Loss:543.943229675293 | Loss: 2.7062 | Acc_train: 41.63% | Batch: 0.32\n",
      "Epoch\" 49/55 | Loss:811.7523519992828 | Loss: 2.6969 | Acc_train: 42.00% | Batch: 0.48\n",
      "Epoch\" 49/55 | Loss:1089.9860346317291 | Loss: 2.7182 | Acc_train: 41.59% | Batch: 0.64\n",
      "Epoch\" 49/55 | Loss:1366.7832090854645 | Loss: 2.7281 | Acc_train: 41.26% | Batch: 0.8\n",
      "Epoch\" 49/55 | Loss:1641.4515562057495 | Loss: 2.7312 | Acc_train: 41.22% | Batch: 0.96\n",
      "Epoch\" 50/55 | Loss:270.41499876976013 | Loss: 2.6774 | Acc_train: 42.97% | Batch: 0.16\n",
      "Epoch\" 50/55 | Loss:544.187338590622 | Loss: 2.7074 | Acc_train: 42.28% | Batch: 0.32\n",
      "Epoch\" 50/55 | Loss:817.3606543540955 | Loss: 2.7155 | Acc_train: 42.12% | Batch: 0.48\n",
      "Epoch\" 50/55 | Loss:1093.4189283847809 | Loss: 2.7267 | Acc_train: 41.75% | Batch: 0.64\n",
      "Epoch\" 50/55 | Loss:1366.0980052947998 | Loss: 2.7267 | Acc_train: 41.73% | Batch: 0.8\n",
      "Epoch\" 50/55 | Loss:1638.8222029209137 | Loss: 2.7268 | Acc_train: 41.67% | Batch: 0.96\n",
      "Epoch\" 51/55 | Loss:271.58963322639465 | Loss: 2.6890 | Acc_train: 42.31% | Batch: 0.16\n",
      "Epoch\" 51/55 | Loss:542.3541216850281 | Loss: 2.6983 | Acc_train: 42.62% | Batch: 0.32\n",
      "Epoch\" 51/55 | Loss:816.1972599029541 | Loss: 2.7116 | Acc_train: 42.37% | Batch: 0.48\n",
      "Epoch\" 51/55 | Loss:1085.3564715385437 | Loss: 2.7066 | Acc_train: 42.40% | Batch: 0.64\n",
      "Epoch\" 51/55 | Loss:1360.6204471588135 | Loss: 2.7158 | Acc_train: 42.16% | Batch: 0.8\n",
      "Epoch\" 51/55 | Loss:1634.9996318817139 | Loss: 2.7205 | Acc_train: 41.96% | Batch: 0.96\n",
      "Epoch\" 52/55 | Loss:268.9465730190277 | Loss: 2.6628 | Acc_train: 43.25% | Batch: 0.16\n",
      "Epoch\" 52/55 | Loss:542.2956094741821 | Loss: 2.6980 | Acc_train: 42.09% | Batch: 0.32\n",
      "Epoch\" 52/55 | Loss:813.7828018665314 | Loss: 2.7036 | Acc_train: 42.08% | Batch: 0.48\n",
      "Epoch\" 52/55 | Loss:1088.3837399482727 | Loss: 2.7142 | Acc_train: 41.65% | Batch: 0.64\n",
      "Epoch\" 52/55 | Loss:1359.5298190116882 | Loss: 2.7136 | Acc_train: 41.68% | Batch: 0.8\n",
      "Epoch\" 52/55 | Loss:1636.3788557052612 | Loss: 2.7228 | Acc_train: 41.52% | Batch: 0.96\n",
      "Epoch\" 53/55 | Loss:270.09095573425293 | Loss: 2.6742 | Acc_train: 42.53% | Batch: 0.16\n",
      "Epoch\" 53/55 | Loss:540.8672926425934 | Loss: 2.6909 | Acc_train: 42.20% | Batch: 0.32\n",
      "Epoch\" 53/55 | Loss:814.7530686855316 | Loss: 2.7068 | Acc_train: 41.97% | Batch: 0.48\n",
      "Epoch\" 53/55 | Loss:1086.0590665340424 | Loss: 2.7084 | Acc_train: 41.87% | Batch: 0.64\n",
      "Epoch\" 53/55 | Loss:1362.3545861244202 | Loss: 2.7193 | Acc_train: 41.47% | Batch: 0.8\n",
      "Epoch\" 53/55 | Loss:1633.7421176433563 | Loss: 2.7184 | Acc_train: 41.62% | Batch: 0.96\n",
      "Epoch\" 54/55 | Loss:269.20950841903687 | Loss: 2.6654 | Acc_train: 41.86% | Batch: 0.16\n",
      "Epoch\" 54/55 | Loss:536.8134741783142 | Loss: 2.6707 | Acc_train: 42.48% | Batch: 0.32\n",
      "Epoch\" 54/55 | Loss:809.0971896648407 | Loss: 2.6880 | Acc_train: 42.21% | Batch: 0.48\n",
      "Epoch\" 54/55 | Loss:1080.955556154251 | Loss: 2.6956 | Acc_train: 42.09% | Batch: 0.64\n",
      "Epoch\" 54/55 | Loss:1354.1914649009705 | Loss: 2.7030 | Acc_train: 41.79% | Batch: 0.8\n",
      "Epoch\" 54/55 | Loss:1628.11545753479 | Loss: 2.7090 | Acc_train: 41.82% | Batch: 0.96\n",
      "Epoch\" 55/55 | Loss:270.068639755249 | Loss: 2.6739 | Acc_train: 42.16% | Batch: 0.16\n",
      "Epoch\" 55/55 | Loss:541.1318714618683 | Loss: 2.6922 | Acc_train: 41.84% | Batch: 0.32\n",
      "Epoch\" 55/55 | Loss:815.2422895431519 | Loss: 2.7084 | Acc_train: 41.45% | Batch: 0.48\n",
      "Epoch\" 55/55 | Loss:1084.8767755031586 | Loss: 2.7054 | Acc_train: 41.65% | Batch: 0.64\n",
      "Epoch\" 55/55 | Loss:1355.3355004787445 | Loss: 2.7053 | Acc_train: 41.75% | Batch: 0.8\n",
      "Epoch\" 55/55 | Loss:1625.797070980072 | Loss: 2.7052 | Acc_train: 41.88% | Batch: 0.96\n"
     ]
    }
   ],
   "source": [
    "model_train(model,train_loader, criterion, optimizer)\n",
    "val_loss, val_acc = model_val(model, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "32a901e0-788e-42de-9bc0-c5a3d99005e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val_loss: 0.0164, Val_acc: 51.5625%\n"
     ]
    }
   ],
   "source": [
    "print(f'Val_loss: {val_loss:.4f}, Val_acc: {val_acc}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "3034e87e-bce0-4335-b463-5433cad4f112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.016890619970430995 | test acc: 40.625%\n"
     ]
    }
   ],
   "source": [
    "def model_test(model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, targets in test_loader:\n",
    "        with torch.no_grad():\n",
    "            images,targets = images.to(device), targets.to(device)\n",
    "            outputs = model(images)\n",
    "\n",
    "            loss = criterion(outputs, targets)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            _, predicted = outputs.max(1)\n",
    "            total = predicted.size(0)\n",
    "            correct = predicted.eq(targets).sum().item()\n",
    "            return test_loss/len(test_loader), 100.*correct/total\n",
    "test_loss,test_acc = model_test(model, test_loader)\n",
    "print(f\"test loss: {test_loss} | test acc: {test_acc}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "6f6122a6-f4e7-4a2c-a989-68d460a9f9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------- 5 epochs --------------- lr = 3e-3\n",
    "#Acc_train: 26.01%. Val_acc: 28.125% test acc: 29.6875% - no weight_decay/ SGD opimizer\n",
    "#Acc_train: 25.56%  Val_acc: 25.0%   test acc: 35.9375% - AdamW optimizer\n",
    "#Acc_train: 25.09%  Val_acc: 25.0%   test acc: 32.8125% - AdamW with 1e-2 weight_decay\n",
    "# ------------- 5 epochs --------------- lr = 3e-4\n",
    "#Acc_train: 28.55%  Val_acc: 28.125% test acc: 23.4375%\n",
    "# -------------- 40 epochs -------------- lr = 3e-3\n",
    "#Acc_train: 41.29%  Val_acc: 56.25%   test acc: 48.4375% - AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "ce9ffe73-553d-4741-bb81-6d304de483c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "new_cifar_data = torchvision.datasets.CIFAR100(root=path, download=True, transform=transform)\n",
    "new_cifar_loader = DataLoader(new_cifar_data, batch_size=32, pin_memory=4, shuffle=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "fb25113f-5526-400f-a99b-300ba56f8fd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: 89/19\n",
      "Predicted class: 29/29\n",
      "Predicted class: 0/0\n",
      "Predicted class: 11/11\n",
      "Predicted class: 1/1\n",
      "Predicted class: 22/86\n",
      "Predicted class: 27/90\n",
      "Predicted class: 10/28\n",
      "Predicted class: 71/23\n",
      "Predicted class: 31/31\n",
      "Predicted class: 39/39\n",
      "Predicted class: 52/96\n",
      "Predicted class: 82/82\n",
      "Predicted class: 37/17\n",
      "Predicted class: 71/71\n",
      "correct/total: 8/15\n",
      "Accuracy:53.333333333333336%\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "x = 15\n",
    "for i in range(x):\n",
    "    total = x    \n",
    "    image, label = new_cifar_data[i]\n",
    "    image = image.unsqueeze(0)\n",
    "    model.to('cpu')\n",
    "    model.eval()\n",
    "    output = model(image)\n",
    "    predicted = torch.argmax(output, dim=1).item()\n",
    "    if  predicted == label:\n",
    "        correct += 1\n",
    "    \n",
    "    print(f'Predicted class: {predicted}/{label}')\n",
    "accuracy = 100.*correct/total\n",
    "print(f'correct/total: {correct}/{total}')\n",
    "print(f'Accuracy:{accuracy}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7b38a2-858f-4a00-bd92-4b3bc259cd45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
